{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d264c2f0-ceb0-43c4-8a00-e04b1e8c0651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 238\n",
      "1 297\n",
      "2 927\n",
      "3 933\n",
      "4 179\n",
      "5 375\n",
      "6 820\n",
      "7 618\n",
      "8 561\n",
      "9 57\n",
      "10 577\n",
      "11 59\n",
      "12 73\n",
      "13 107\n",
      "14 53\n",
      "15 91\n",
      "16 893\n",
      "17 810\n",
      "18 170\n",
      "19 53\n",
      "20 68\n",
      "21 9\n",
      "22 1\n",
      "23 92\n",
      "24 9\n",
      "25 8\n",
      "26 9\n",
      "27 308\n",
      "28 447\n",
      "29 392\n",
      "30 107\n",
      "31 42\n",
      "32 4\n",
      "33 45\n",
      "34 141\n",
      "35 110\n",
      "36 3\n",
      "37 758\n",
      "38 9\n",
      "39 9\n",
      "40 388\n",
      "41 220\n",
      "42 644\n",
      "43 649\n",
      "44 499\n",
      "45 2\n",
      "46 937\n",
      "47 169\n",
      "48 286\n",
      "49 2\n"
     ]
    }
   ],
   "source": [
    "# Listing 5.5 summarize the number of unique values for each column using numpy\n",
    "from numpy import unique\n",
    "import pandas\n",
    "\n",
    "#read dataset\n",
    "dataframe = pandas.read_csv(\"D:/data/oil-spill.csv\", header=None)\n",
    "data = dataframe.values\n",
    "\n",
    "# summarize the number of unique values in each column\n",
    "for i in range(data.shape[1]):\n",
    "    print(i, len(unique(data[:, i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ec28c1-f8eb-48ba-9757-e56f388c2de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 50)\n",
      "[22]\n",
      "(937, 49)\n"
     ]
    }
   ],
   "source": [
    "# Listing 5.7 delete columns with a single unique value\n",
    "# delete columns with a single unique value\n",
    "from pandas import read_csv\n",
    "# load the dataset\n",
    "#read dataset\n",
    "dataframe = pandas.read_csv(\"D:/data/oil-spill.csv\", header=None)\n",
    "data = dataframe.values\n",
    "print(dataframe.shape)\n",
    "\n",
    "# get number of unique values for each column\n",
    "counts = dataframe.nunique()\n",
    "\n",
    "#print(counts)\n",
    "\n",
    "# record columns to delete\n",
    "to_del = [i for i,v in enumerate(counts) if v == 1]\n",
    "print(to_del)\n",
    "\n",
    "# drop useless columns\n",
    "dataframe.drop(to_del, axis=1, inplace=True)\n",
    "print(dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2c12dd0-fcb2-4f4c-866c-0888d2ecf2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 50)\n",
      "[22, 36, 45, 49]\n",
      "(937, 46)\n",
      "      0      1        2       3    4           5      6      7        8   \\\n",
      "0      1   2558  1506.09  456.63   90   6395000.0  40.88   7.89  29780.0   \n",
      "1      2  22325    79.11  841.03  180  55812500.0  51.11   1.21  61900.0   \n",
      "2      3    115  1449.85  608.43   88    287500.0  40.42   7.34   3340.0   \n",
      "3      4   1201  1562.53  295.65   66   3002500.0  42.40   7.97  18030.0   \n",
      "4      5    312   950.27  440.86   37    780000.0  41.43   7.03   3350.0   \n",
      "..   ...    ...      ...     ...  ...         ...    ...    ...      ...   \n",
      "932  200     12    92.42  364.42  135     97200.0  59.42  10.34    884.0   \n",
      "933  201     11    98.82  248.64  159     89100.0  59.64  10.18    831.0   \n",
      "934  202     14    25.14  428.86   24    113400.0  60.14  17.94    847.0   \n",
      "935  203     10    96.00  451.30   68     81000.0  59.90  15.01    831.0   \n",
      "936  204     11     7.73  235.73  135     89100.0  61.82  12.24    831.0   \n",
      "\n",
      "       9   ...  38  39       40        41       42       43     44        46  \\\n",
      "0    0.19  ...  89  69  2850.00   1000.00   763.16   135.46   3.73  33243.19   \n",
      "1    0.02  ...  89  69  5750.00  11500.00  9593.48  1648.80   0.60  51572.04   \n",
      "2    0.18  ...  89  69  1400.00    250.00   150.00    45.13   9.33  31692.84   \n",
      "3    0.19  ...  89  69  6041.52    761.58   453.21   144.97  13.33  37696.21   \n",
      "4    0.17  ...  89  69  1320.04    710.63   512.54   109.16   2.58  29038.17   \n",
      "..    ...  ...  ..  ..      ...       ...      ...      ...    ...       ...   \n",
      "932  0.17  ...  82  50   381.84    254.56    84.85   146.97   4.50   2593.50   \n",
      "933  0.17  ...  82  50   284.60    180.00   150.00    51.96   1.90   4361.25   \n",
      "934  0.30  ...  82  50   402.49    180.00   180.00     0.00   2.24   2153.05   \n",
      "935  0.25  ...  82  50   402.49    180.00    90.00    73.48   4.47   2421.43   \n",
      "936  0.20  ...  82  50   254.56    254.56   127.28   180.00   2.00   3782.68   \n",
      "\n",
      "        47    48  \n",
      "0    65.74  7.95  \n",
      "1    65.73  6.26  \n",
      "2    65.81  7.84  \n",
      "3    65.67  8.07  \n",
      "4    65.66  7.35  \n",
      "..     ...   ...  \n",
      "932  65.85  6.39  \n",
      "933  65.70  6.53  \n",
      "934  65.91  6.12  \n",
      "935  65.97  6.32  \n",
      "936  65.65  6.26  \n",
      "\n",
      "[937 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "# Listing 5.7 delete columns with a single unique value\n",
    "# delete columns with a single unique value\n",
    "from pandas import read_csv\n",
    "# load the dataset\n",
    "#read dataset\n",
    "dataframe = pandas.read_csv(\"D:/data/oil-spill.csv\", header=None)\n",
    "data = dataframe.values\n",
    "print(dataframe.shape)\n",
    "\n",
    "# get number of unique values for each column\n",
    "counts = dataframe.nunique()\n",
    "\n",
    "#print(counts)\n",
    "\n",
    "# record columns to delete\n",
    "to_del = [i for i,v in enumerate(counts) if v <= 3]\n",
    "print(to_del)\n",
    "\n",
    "# drop useless columns\n",
    "dataframe.drop(to_del, axis=1, inplace=True)\n",
    "print(dataframe.shape)\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e612a33b-fd80-49ed-8a25-a53324c1df34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 238, 25.4%\n",
      "1, 297, 31.7%\n",
      "2, 927, 98.9%\n",
      "3, 933, 99.6%\n",
      "4, 179, 19.1%\n",
      "5, 375, 40.0%\n",
      "6, 820, 87.5%\n",
      "7, 618, 66.0%\n",
      "8, 561, 59.9%\n",
      "9, 57, 6.1%\n",
      "10, 577, 61.6%\n",
      "11, 59, 6.3%\n",
      "12, 73, 7.8%\n",
      "13, 107, 11.4%\n",
      "14, 53, 5.7%\n",
      "15, 91, 9.7%\n",
      "16, 893, 95.3%\n",
      "17, 810, 86.4%\n",
      "18, 170, 18.1%\n",
      "19, 53, 5.7%\n",
      "20, 68, 7.3%\n",
      "21, 9, 1.0%\n",
      "22, 1, 0.1%\n",
      "23, 92, 9.8%\n",
      "24, 9, 1.0%\n",
      "25, 8, 0.9%\n",
      "26, 9, 1.0%\n",
      "27, 308, 32.9%\n",
      "28, 447, 47.7%\n",
      "29, 392, 41.8%\n",
      "30, 107, 11.4%\n",
      "31, 42, 4.5%\n",
      "32, 4, 0.4%\n",
      "33, 45, 4.8%\n",
      "34, 141, 15.0%\n",
      "35, 110, 11.7%\n",
      "36, 3, 0.3%\n",
      "37, 758, 80.9%\n",
      "38, 9, 1.0%\n",
      "39, 9, 1.0%\n",
      "40, 388, 41.4%\n",
      "41, 220, 23.5%\n",
      "42, 644, 68.7%\n",
      "43, 649, 69.3%\n",
      "44, 499, 53.3%\n",
      "45, 2, 0.2%\n",
      "46, 937, 100.0%\n",
      "47, 169, 18.0%\n",
      "48, 286, 30.5%\n",
      "49, 2, 0.2%\n"
     ]
    }
   ],
   "source": [
    "# Listing 5.9 summarize the percentage of unique values for each column using numpy\n",
    "from numpy import unique\n",
    "import pandas\n",
    "\n",
    "#read dataset\n",
    "dataframe = pandas.read_csv(\"D:/data/oil-spill.csv\", header=None)\n",
    "data = dataframe.values\n",
    "\n",
    "# summarize the number of unique values in each column\n",
    "for i in range(data.shape[1]):\n",
    "    num = len(unique(data[:, i]))\n",
    "    percentage = float(num) / data.shape[0] * 100\n",
    "    print('%d, %d, %.1f%%' % (i, num, percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c7e181-399f-42b4-a996-e5af945bda31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21, 9, 1.0%\n",
      "22, 1, 0.1%\n",
      "24, 9, 1.0%\n",
      "25, 8, 0.9%\n",
      "26, 9, 1.0%\n",
      "32, 4, 0.4%\n",
      "36, 3, 0.3%\n",
      "38, 9, 1.0%\n",
      "39, 9, 1.0%\n",
      "45, 2, 0.2%\n",
      "49, 2, 0.2%\n"
     ]
    }
   ],
   "source": [
    "# Listing 5.11 summarize the percentage of unique values for each column using numpy\n",
    "# indexing less than 1 parcent \n",
    "from numpy import unique\n",
    "import pandas\n",
    "\n",
    "#read dataset\n",
    "dataframe = pandas.read_csv(\"D:/data/oil-spill.csv\", header=None)\n",
    "data = dataframe.values\n",
    "\n",
    "# summarize the number of unique values in each column\n",
    "for i in range(data.shape[1]):\n",
    "    num = len(unique(data[:, i]))\n",
    "    percentage = float(num) / data.shape[0] * 100\n",
    "    if percentage < 1: # Condition less than 1 percent\n",
    "        print('%d, %d, %.1f%%' % (i, num, percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aa0e4d3-c8e4-4178-91ae-93ca5deb2756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 50)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'nunique'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# get number of unique values for each column\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m counts \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mnunique()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# record columns to delete\u001b[39;00m\n\u001b[0;32m     14\u001b[0m to_del \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i,v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(counts) \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mfloat\u001b[39m(v)\u001b[38;5;241m/\u001b[39mdf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'nunique'"
     ]
    }
   ],
   "source": [
    "# delete columns where number of unique values is less than 1% of the rows\n",
    "from numpy import unique\n",
    "import pandas\n",
    "\n",
    "#read dataset\n",
    "dataframe = pandas.read_csv(\"D:/data/oil-spill.csv\", header=None)\n",
    "df = dataframe.values\n",
    "print(df.shape)\n",
    "\n",
    "# get number of unique values for each column\n",
    "counts = df.nunique()\n",
    "\n",
    "# record columns to delete\n",
    "to_del = [i for i,v in enumerate(counts) if (float(v)/df.shape[0]*100) < 1]\n",
    "print(to_del)\n",
    "\n",
    "# drop useless columns\n",
    "df.drop(to_del, axis=1, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20cd7cea-7354-40d8-936e-7f2ecf755e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 50)\n",
      "[21, 22, 24, 25, 26, 32, 36, 38, 39, 45, 49]\n",
      "(937, 39)\n"
     ]
    }
   ],
   "source": [
    "# delete columns where number of unique values is less than 1% of the rows\n",
    "import pandas as pd\n",
    "from numpy import unique\n",
    "\n",
    "# Read dataset\n",
    "dataframe = pd.read_csv(\"D:/data/oil-spill.csv\", header=None)\n",
    "print(dataframe.shape)\n",
    "\n",
    "# Get number of unique values for each column\n",
    "counts = dataframe.nunique()\n",
    "\n",
    "# Record columns to delete (less than 1% unique values)\n",
    "to_del = [i for i, v in enumerate(counts) if (float(v)/dataframe.shape[0]*100) < 1]\n",
    "print(to_del)\n",
    "\n",
    "# Drop useless columns\n",
    "dataframe.drop(to_del, axis=1, inplace=True)\n",
    "print(dataframe.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5344bf1b-c06c-4db4-86ba-e77288bf34b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      False\n",
      "1      False\n",
      "2      False\n",
      "3      False\n",
      "4      False\n",
      "       ...  \n",
      "145    False\n",
      "146    False\n",
      "147    False\n",
      "148    False\n",
      "149    False\n",
      "Length: 150, dtype: bool\n",
      "True\n",
      "       0    1    2    3               4\n",
      "34   4.9  3.1  1.5  0.1     Iris-setosa\n",
      "37   4.9  3.1  1.5  0.1     Iris-setosa\n",
      "142  5.8  2.7  5.1  1.9  Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "# Listing 5.22 locate rows of duplicate data\n",
    "from pandas import read_csv\n",
    "\n",
    "# load the dataset\n",
    "df = read_csv('D:/data/iris.csv', header=None)\n",
    "\n",
    "# calculate duplicates\n",
    "dups = df.duplicated()\n",
    "\n",
    "print(dups)\n",
    "\n",
    "# report if there are any duplicates\n",
    "print(dups.any())\n",
    "\n",
    "# list all duplicate rows\n",
    "print(df[dups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fd142af-272f-43e0-bf62-b54bbdc3825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 5)\n",
      "(147, 5)\n"
     ]
    }
   ],
   "source": [
    "# Listing 5.24 delete rows of duplicate data from the dataset\n",
    "from pandas import read_csv\n",
    "\n",
    "# load the dataset\n",
    "df = read_csv('D:/data/iris.csv', header=None)\n",
    "print(df.shape)\n",
    "\n",
    "# delete duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d898f82-bc93-44bb-9f10-a83ecf8671f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_3920\\943626426.py:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  dataset = read_csv(filename, delim_whitespace=True, names=names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(338, 0)) while a minimum of 1 is required by LinearRegression.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# fit the model\u001b[39;00m\n\u001b[0;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[1;32m---> 24\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# evaluate the model\u001b[39;00m\n\u001b[0;32m     27\u001b[0m yhat \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:609\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    605\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[0;32m    607\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 609\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    610\u001b[0m     X,\n\u001b[0;32m    611\u001b[0m     y,\n\u001b[0;32m    612\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m    613\u001b[0m     y_numeric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    614\u001b[0m     multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    615\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    616\u001b[0m )\n\u001b[0;32m    618\u001b[0m has_sw \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_sw:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1301\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1296\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1299\u001b[0m     )\n\u001b[1;32m-> 1301\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1302\u001b[0m     X,\n\u001b[0;32m   1303\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1304\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1305\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1306\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1307\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1308\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39mforce_writeable,\n\u001b[0;32m   1309\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1310\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1311\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1312\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1313\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1314\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1315\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1316\u001b[0m )\n\u001b[0;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1320\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1096\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1094\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m<\u001b[39m ensure_min_features:\n\u001b[1;32m-> 1096\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1097\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m feature(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1098\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m a minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1099\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_features, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_features, context)\n\u001b[0;32m   1100\u001b[0m         )\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_writeable:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;66;03m# By default, array.copy() creates a C-ordered copy. We set order=K to\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;66;03m# preserve the order of the array.\u001b[39;00m\n\u001b[0;32m   1105\u001b[0m     copy_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(array) \u001b[38;5;28;01melse\u001b[39;00m {}\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(338, 0)) while a minimum of 1 is required by LinearRegression."
     ]
    }
   ],
   "source": [
    "# Listing 6.17 evaluate model on the raw dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "filename = 'D:/data/housing/housing.csv'\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "dataset = read_csv(filename, delim_whitespace=True, names=names)\n",
    "\n",
    "# shape\n",
    "print(dataset.shape)\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99034433-06b1-456e-8395-8e92ead0ce03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_3920\\4032982711.py:8: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = read_csv(\"D:/data/housing/housing.csv\", delim_whitespace=True)\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.17 evaluate model on the raw dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# load the dataset\n",
    "df = read_csv(\"D:/data/housing/housing.csv\", delim_whitespace=True)\n",
    "\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d949340c-f424-4e20-8a0a-71a566c4e0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(338, 13) (338,)\n",
      "(291, 13) (291,)\n",
      "MAE: 3.590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_3920\\3269428361.py:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = read_csv(\"D:/data/housing/housing.csv\", delim_whitespace=True)\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.21 evaluate model on training dataset with outliers removed\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# load the dataset\n",
    "df = read_csv(\"D:/data/housing/housing.csv\", delim_whitespace=True)\n",
    "\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# identify outliers in the training dataset\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "#print(yhat)\n",
    "\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "#print(mask)\n",
    "\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ecc81c-3d2d-479c-b488-aad20c151303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
